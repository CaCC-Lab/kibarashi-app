# クイックスタート実装ガイド
## 革新的機能の即座実装のための技術指南

最終更新: 2025年6月29日  
対象: 開発者向け即座実装ガイド

---

## 🚀 今すぐ実装開始！

### 1分で開始できる最優先実装

```bash
# Phase 4.1 開発ブランチ作成
git checkout -b feature/phase-4-innovations

# 必要依存関係の一括インストール
npm install @tensorflow/tfjs natural sentiment compromise ssml-builder
npm install --save-dev @types/dom-speech-recognition

# 新機能ディレクトリ構造作成
mkdir -p src/features/{cognitive-reframing,mood-input,smart-nudging}
mkdir -p src/components/advanced-ui/{emotion-canvas,voice-controls}
mkdir -p src/services/{ml-prediction,advanced-tts,privacy-engine}
mkdir -p src/utils/{emotion-analysis,behavioral-prediction}

echo "🎯 Phase 4.1 開発環境準備完了！"
```

---

## 🔥 最優先実装: ネガティブ感情リフレーミングエンジン

### Step 1: 基盤型定義 (5分)

```typescript
// src/features/cognitive-reframing/types.ts
export interface EmotionReframingRequest {
  emotionType: 'イライラ' | 'モヤモヤ' | '不安' | '疲れ' | '落ち込み';
  situation: 'meeting' | 'email' | 'presentation' | 'deadline' | 'interpersonal';
  intensity: number; // 1-10 scale
  availableTime: number; // minutes
  workContext: {
    isInOffice: boolean;
    canUseAudio: boolean;
    privacyLevel: 'high' | 'medium' | 'low';
  };
}

export interface CognitiveReframingStrategy {
  name: string;
  description: string;
  technique: 'evidence-challenge' | 'perspective-shift' | 'self-compassion' | 'action-focus';
  targetEmotions: string[];
  estimatedEffectiveness: number; // 0-1
}

export interface ReframingResponse {
  strategy: CognitiveReframingStrategy;
  personalizedScript: string;
  guidedQuestions: string[];
  breathingCue: string;
  affirmation: string;
  audioSSML?: string;
  followUpSuggestions: string[];
}
```

### Step 2: CBT戦略実装 (15分)

```typescript
// src/features/cognitive-reframing/strategies.ts
export const CBT_STRATEGIES: Record<string, CognitiveReframingStrategy> = {
  evidenceChallenge: {
    name: "事実検証法",
    description: "感情の根拠となる事実を冷静に検証する",
    technique: "evidence-challenge",
    targetEmotions: ["イライラ", "不安", "落ち込み"],
    estimatedEffectiveness: 0.85
  },
  
  perspectiveShift: {
    name: "視点転換法", 
    description: "他者の視点や将来の視点から状況を見直す",
    technique: "perspective-shift",
    targetEmotions: ["モヤモヤ", "イライラ", "落ち込み"],
    estimatedEffectiveness: 0.78
  },
  
  selfCompassion: {
    name: "自己慈悲法",
    description: "自分自身に優しく接し、完璧主義を手放す", 
    technique: "self-compassion",
    targetEmotions: ["落ち込み", "疲れ", "不安"],
    estimatedEffectiveness: 0.82
  },
  
  actionFocus: {
    name: "行動転換法",
    description: "感情を建設的な行動エネルギーに変換する",
    technique: "action-focus", 
    targetEmotions: ["イライラ", "モヤモヤ", "疲れ"],
    estimatedEffectiveness: 0.75
  }
};

export class CognitiveReframingEngine {
  selectOptimalStrategy(request: EmotionReframingRequest): CognitiveReframingStrategy {
    const applicableStrategies = Object.values(CBT_STRATEGIES)
      .filter(strategy => strategy.targetEmotions.includes(request.emotionType))
      .sort((a, b) => b.estimatedEffectiveness - a.estimatedEffectiveness);
    
    // 状況に応じた戦略調整
    if (request.situation === 'meeting' && request.intensity > 7) {
      return applicableStrategies.find(s => s.technique === 'evidence-challenge') || applicableStrategies[0];
    }
    
    if (request.availableTime < 3) {
      return applicableStrategies.find(s => s.technique === 'self-compassion') || applicableStrategies[0];
    }
    
    return applicableStrategies[0];
  }
}
```

### Step 3: Gemini統合実装 (10分)

```typescript
// src/features/cognitive-reframing/gemini-integration.ts
export class GeminiReframingService {
  private readonly REFRAMING_PROMPTS = {
    evidenceChallenge: `
あなたは職場のストレス解消に特化した認知行動療法のエキスパートです。

ユーザーの状況:
- 感情: {emotionType}
- 状況: {situation}  
- 強度: {intensity}/10
- 利用可能時間: {availableTime}分

以下の形式で、優しく共感的で実用的なリフレーミングガイドを作成してください:

1. 共感的な声かけ (20文字以内)
2. 事実の整理を促す質問 (1つ)
3. 別の視点の提示 (具体的で建設的)
4. 次のアクションへの誘導 (職場で実践可能)
5. 励ましの言葉 (15文字以内)

口調: 親しみやすく、押し付けがましくない
制約: 職場で{availableTime}分以内に実践可能な内容
`,

    perspectiveShift: `
職場でのストレス状況に対して、視点転換を通じた心理的サポートを提供してください。

現在の状況: {emotionType}を{intensity}/10の強度で感じている
状況詳細: {situation}
時間制約: {availableTime}分

以下のステップで視点転換ガイドを作成:

1. 現在の感情への共感 (「大変でしたね」など)
2. 時間軸の拡張 ("1年後には..." "この経験が...")
3. 他者視点の導入 ("もし同僚が同じ状況なら...")
4. 学習機会としての再定義
5. 前向きな行動への誘導

必須: 日本の職場文化に配慮し、自然で受け入れやすい表現を使用
`
  };

  async generateReframing(request: EmotionReframingRequest): Promise<ReframingResponse> {
    const strategy = new CognitiveReframingEngine().selectOptimalStrategy(request);
    const prompt = this.REFRAMING_PROMPTS[strategy.technique];
    
    const personalizedPrompt = this.substituteVariables(prompt, request);
    
    try {
      const geminiResponse = await fetch('/api/gemini/reframing', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt: personalizedPrompt })
      });
      
      const content = await geminiResponse.json();
      
      return {
        strategy,
        personalizedScript: content.script,
        guidedQuestions: content.questions,
        breathingCue: this.generateBreathingCue(request.intensity),
        affirmation: content.affirmation,
        audioSSML: this.generateSSML(content.script, request),
        followUpSuggestions: content.followUp
      };
    } catch (error) {
      // フォールバック: 事前定義されたテンプレート
      return this.getFallbackReframing(request, strategy);
    }
  }

  private generateBreathingCue(intensity: number): string {
    if (intensity >= 8) return "深く息を吸って(4秒)、ゆっくり吐いてください(8秒)";
    if (intensity >= 5) return "自然な呼吸に意識を向けて、3回深呼吸しましょう";
    return "軽く深呼吸をして、肩の力を抜いてください";
  }

  private generateSSML(script: string, request: EmotionReframingRequest): string {
    const rate = request.intensity > 7 ? 'slow' : 'medium';
    const pitch = request.emotionType === '落ち込み' ? '+2Hz' : 'default';
    
    return `
      <speak>
        <prosody rate="${rate}" pitch="${pitch}">
          <p>
            <s>${script.split('。')[0]}。</s>
            <break time="1s"/>
            <s>${script.split('。').slice(1).join('。')}</s>
          </p>
        </prosody>
      </speak>
    `;
  }
}
```

---

## 🎨 次優先: ゼロUI・ムード入力システム

### Step 1: 感情座標コンポーネント (10分)

```typescript
// src/components/advanced-ui/emotion-canvas/EmotionCanvas.tsx
import React, { useRef, useEffect, useState, useCallback } from 'react';

interface EmotionalCoordinates {
  valence: number; // -1.0 (negative) to +1.0 (positive)
  arousal: number; // -1.0 (calm) to +1.0 (energetic)
}

interface EmotionCanvasProps {
  onEmotionChange: (emotion: EmotionalCoordinates) => void;
  onSelectionComplete: (emotion: EmotionalCoordinates) => void;
}

export const EmotionCanvas: React.FC<EmotionCanvasProps> = ({
  onEmotionChange,
  onSelectionComplete
}) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const [currentEmotion, setCurrentEmotion] = useState<EmotionalCoordinates>({ valence: 0, arousal: 0 });
  const [isInteracting, setIsInteracting] = useState(false);

  const drawCanvas = useCallback(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;

    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    const { width, height } = canvas;
    const centerX = width / 2;
    const centerY = height / 2;

    // Clear canvas
    ctx.clearRect(0, 0, width, height);

    // Draw background gradient
    const gradient = ctx.createRadialGradient(centerX, centerY, 0, centerX, centerY, Math.max(width, height) / 2);
    gradient.addColorStop(0, 'rgba(124, 58, 237, 0.1)');
    gradient.addColorStop(1, 'rgba(124, 58, 237, 0.05)');
    ctx.fillStyle = gradient;
    ctx.fillRect(0, 0, width, height);

    // Draw axis lines
    ctx.strokeStyle = 'rgba(156, 163, 175, 0.3)';
    ctx.lineWidth = 1;
    ctx.setLineDash([5, 5]);
    
    // Horizontal line (valence axis)
    ctx.beginPath();
    ctx.moveTo(0, centerY);
    ctx.lineTo(width, centerY);
    ctx.stroke();

    // Vertical line (arousal axis)
    ctx.beginPath();
    ctx.moveTo(centerX, 0);
    ctx.lineTo(centerX, height);
    ctx.stroke();

    ctx.setLineDash([]);

    // Draw quadrant labels
    ctx.fillStyle = 'rgba(107, 114, 128, 0.6)';
    ctx.font = '12px Inter, system-ui, sans-serif';
    ctx.textAlign = 'center';
    
    ctx.fillText('エネルギッシュ', centerX, 20);
    ctx.fillText('穏やか', centerX, height - 10);
    
    ctx.save();
    ctx.translate(15, centerY);
    ctx.rotate(-Math.PI / 2);
    ctx.fillText('ネガティブ', 0, 0);
    ctx.restore();
    
    ctx.save();
    ctx.translate(width - 15, centerY);
    ctx.rotate(Math.PI / 2);
    ctx.fillText('ポジティブ', 0, 0);
    ctx.restore();

    // Draw emotion indicators in quadrants
    const indicators = [
      { text: '興奮・活力', x: centerX + 60, y: centerY - 60, emoji: '⚡' },
      { text: 'リラックス・満足', x: centerX + 60, y: centerY + 60, emoji: '😌' },
      { text: 'イライラ・焦り', x: centerX - 60, y: centerY - 60, emoji: '😤' },
      { text: '落ち込み・疲れ', x: centerX - 60, y: centerY + 60, emoji: '😔' }
    ];

    ctx.font = '10px Inter, system-ui, sans-serif';
    ctx.textAlign = 'center';
    indicators.forEach(indicator => {
      ctx.fillStyle = 'rgba(107, 114, 128, 0.4)';
      ctx.fillText(indicator.emoji, indicator.x, indicator.y - 10);
      ctx.fillText(indicator.text, indicator.x, indicator.y + 5);
    });

    // Draw current position
    const x = centerX + currentEmotion.valence * (width / 2 - 20);
    const y = centerY - currentEmotion.arousal * (height / 2 - 20);

    // Glow effect
    const glowGradient = ctx.createRadialGradient(x, y, 0, x, y, 20);
    glowGradient.addColorStop(0, 'rgba(124, 58, 237, 0.4)');
    glowGradient.addColorStop(1, 'rgba(124, 58, 237, 0)');
    ctx.fillStyle = glowGradient;
    ctx.fillRect(x - 20, y - 20, 40, 40);

    // Main point
    ctx.beginPath();
    ctx.arc(x, y, isInteracting ? 12 : 8);
    ctx.fillStyle = '#7c3aed';
    ctx.fill();
    
    // Inner highlight
    ctx.beginPath();
    ctx.arc(x, y, isInteracting ? 6 : 4);
    ctx.fillStyle = '#a855f7';
    ctx.fill();

  }, [currentEmotion, isInteracting]);

  useEffect(() => {
    drawCanvas();
  }, [drawCanvas]);

  const handleInteraction = useCallback((clientX: number, clientY: number) => {
    const canvas = canvasRef.current;
    if (!canvas) return;

    const rect = canvas.getBoundingClientRect();
    const x = clientX - rect.left;
    const y = clientY - rect.top;

    const valence = Math.max(-1, Math.min(1, (x - rect.width / 2) / (rect.width / 2 - 20)));
    const arousal = Math.max(-1, Math.min(1, -(y - rect.height / 2) / (rect.height / 2 - 20)));

    const newEmotion = { valence, arousal };
    setCurrentEmotion(newEmotion);
    onEmotionChange(newEmotion);
  }, [onEmotionChange]);

  const handleMouseDown = useCallback((e: React.MouseEvent) => {
    setIsInteracting(true);
    handleInteraction(e.clientX, e.clientY);
  }, [handleInteraction]);

  const handleMouseMove = useCallback((e: React.MouseEvent) => {
    if (!isInteracting) return;
    handleInteraction(e.clientX, e.clientY);
  }, [isInteracting, handleInteraction]);

  const handleMouseUp = useCallback(() => {
    if (isInteracting) {
      setIsInteracting(false);
      onSelectionComplete(currentEmotion);
    }
  }, [isInteracting, currentEmotion, onSelectionComplete]);

  // Touch events
  const handleTouchStart = useCallback((e: React.TouchEvent) => {
    e.preventDefault();
    setIsInteracting(true);
    const touch = e.touches[0];
    handleInteraction(touch.clientX, touch.clientY);
  }, [handleInteraction]);

  const handleTouchMove = useCallback((e: React.TouchEvent) => {
    e.preventDefault();
    if (!isInteracting) return;
    const touch = e.touches[0];
    handleInteraction(touch.clientX, touch.clientY);
  }, [isInteracting, handleInteraction]);

  const handleTouchEnd = useCallback((e: React.TouchEvent) => {
    e.preventDefault();
    if (isInteracting) {
      setIsInteracting(false);
      onSelectionComplete(currentEmotion);
    }
  }, [isInteracting, currentEmotion, onSelectionComplete]);

  return (
    <div className="relative w-full max-w-md mx-auto">
      <canvas
        ref={canvasRef}
        width={320}
        height={320}
        className="w-full h-auto cursor-pointer touch-none border border-gray-200 rounded-xl shadow-sm"
        onMouseDown={handleMouseDown}
        onMouseMove={handleMouseMove}
        onMouseUp={handleMouseUp}
        onMouseLeave={handleMouseUp}
        onTouchStart={handleTouchStart}
        onTouchMove={handleTouchMove}
        onTouchEnd={handleTouchEnd}
      />
      
      <div className="mt-4 text-center">
        <p className="text-sm text-gray-600">
          タップまたはドラッグして現在の気持ちを選択
        </p>
        <div className="mt-2 text-xs text-gray-500">
          選択: <span className="font-medium">
            {currentEmotion.valence > 0.3 ? 'ポジティブ' : 
             currentEmotion.valence < -0.3 ? 'ネガティブ' : '中性'} × {' '}
            {currentEmotion.arousal > 0.3 ? 'エネルギッシュ' : 
             currentEmotion.arousal < -0.3 ? '穏やか' : '普通'}
          </span>
        </div>
      </div>
    </div>
  );
};
```

### Step 2: 瞬時コンテンツ生成 (5分)

```typescript
// src/features/mood-input/InstantContentGenerator.ts
export class InstantContentGenerator {
  private readonly QUADRANT_CONTENT = {
    // 高エネルギー × ポジティブ (右上)
    energeticPositive: {
      title: "この活力を活かしましょう",
      techniques: ["集中力向上呼吸法", "創造性活性化", "効率的タスク処理"],
      audioProfile: "energizing",
      duration: 300
    },
    
    // 低エネルギー × ポジティブ (右下)  
    calmPositive: {
      title: "穏やかな時間を大切に",
      techniques: ["深いリラクゼーション", "感謝の瞑想", "心身の充電"],
      audioProfile: "restoring", 
      duration: 300
    },
    
    // 高エネルギー × ネガティブ (左上)
    energeticNegative: {
      title: "この気持ちを整理しましょう",
      techniques: ["怒りの管理法", "ストレス発散", "感情のリセット"],
      audioProfile: "calming",
      duration: 300
    },
    
    // 低エネルギー × ネガティブ (左下)
    calmNegative: {
      title: "優しく心をケアしませんか",
      techniques: ["自己慈悲の実践", "希望の回復", "エネルギー補充"],
      audioProfile: "nurturing",
      duration: 300
    }
  };

  generateInstantContent(emotion: EmotionalCoordinates): ContentRecommendation {
    const quadrant = this.determineQuadrant(emotion);
    const content = this.QUADRANT_CONTENT[quadrant];
    
    return {
      ...content,
      personalizedMessage: this.generatePersonalizedMessage(emotion),
      immediateAction: this.getImmediateAction(emotion)
    };
  }

  private determineQuadrant(emotion: EmotionalCoordinates): string {
    if (emotion.arousal > 0) {
      return emotion.valence > 0 ? 'energeticPositive' : 'energeticNegative';
    } else {
      return emotion.valence > 0 ? 'calmPositive' : 'calmNegative';
    }
  }

  private generatePersonalizedMessage(emotion: EmotionalCoordinates): string {
    const intensity = Math.sqrt(emotion.valence ** 2 + emotion.arousal ** 2);
    
    if (intensity < 0.3) {
      return "今の気持ちを大切に、そのままの自分を受け入れてください";
    } else if (intensity > 0.7) {
      return "強い感情を感じていますね。一緒に整理していきましょう";
    } else {
      return "今の気持ちに寄り添いながら、心地よい時間を過ごしましょう";
    }
  }
}
```

---

## 🔧 即座実装のためのAPI拡張

### Gemini API拡張 (5分)

```typescript
// api/gemini/reframing.ts (Vercel Function)
import { NextRequest, NextResponse } from 'next/server';

export async function POST(request: NextRequest) {
  try {
    const { prompt } = await request.json();
    
    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${process.env.GEMINI_API_KEY}`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents: [{
          parts: [{
            text: prompt
          }]
        }],
        generationConfig: {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 1024,
        },
        safetySettings: [
          { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_MEDIUM_AND_ABOVE" },
          { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_MEDIUM_AND_ABOVE" },
          { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_MEDIUM_AND_ABOVE" },
          { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_MEDIUM_AND_ABOVE" }
        ]
      })
    });

    const data = await response.json();
    const content = data.candidates[0].content.parts[0].text;
    
    // テキストを構造化してパース
    const structuredResponse = parseReframingResponse(content);
    
    return NextResponse.json(structuredResponse);
    
  } catch (error) {
    console.error('Gemini API Error:', error);
    return NextResponse.json(
      { error: 'リフレーミング生成に失敗しました' },
      { status: 500 }
    );
  }
}

function parseReframingResponse(content: string) {
  // Simple parser - 本格実装では自然言語処理ライブラリを使用
  const lines = content.split('\n').filter(line => line.trim());
  
  return {
    script: lines[0] || "今の気持ちに寄り添いながら、一歩ずつ進んでいきましょう。",
    questions: lines.slice(1, 4).length > 0 ? lines.slice(1, 4) : [
      "この状況で、確実に言えることは何でしょうか？",
      "もし友人が同じ状況なら、どんなアドバイスをしますか？",
      "この経験から学べることがあるとすれば、何でしょうか？"
    ],
    affirmation: lines[lines.length - 1] || "あなたは十分頑張っています。",
    followUp: [
      "少し歩いてみる",
      "温かい飲み物を飲む", 
      "好きな音楽を聞く"
    ]
  };
}
```

---

## 📱 フロントエンド統合実装 (10分)

```typescript
// src/pages/ReframingPage.tsx
import React, { useState, useCallback } from 'react';
import { EmotionCanvas } from '../components/advanced-ui/emotion-canvas/EmotionCanvas';
import { CognitiveReframingEngine, GeminiReframingService } from '../features/cognitive-reframing';

interface EmotionalCoordinates {
  valence: number;
  arousal: number;
}

export const ReframingPage: React.FC = () => {
  const [currentEmotion, setCurrentEmotion] = useState<EmotionalCoordinates>({ valence: 0, arousal: 0 });
  const [isGenerating, setIsGenerating] = useState(false);
  const [reframingContent, setReframingContent] = useState<any>(null);

  const reframingService = new GeminiReframingService();

  const handleEmotionSelection = useCallback(async (emotion: EmotionalCoordinates) => {
    setIsGenerating(true);
    
    try {
      // 感情座標をカテゴリー感情に変換
      const emotionType = convertCoordinatesToEmotion(emotion);
      
      const request = {
        emotionType,
        situation: 'general', // より詳細な状況選択は後続フェーズで実装
        intensity: Math.round(Math.sqrt(emotion.valence ** 2 + emotion.arousal ** 2) * 10),
        availableTime: 5,
        workContext: {
          isInOffice: true,
          canUseAudio: true,
          privacyLevel: 'medium' as const
        }
      };

      const response = await reframingService.generateReframing(request);
      setReframingContent(response);
      
    } catch (error) {
      console.error('Reframing generation failed:', error);
      // フォールバック実装
      setReframingContent(generateFallbackContent(emotion));
    } finally {
      setIsGenerating(false);
    }
  }, [reframingService]);

  const convertCoordinatesToEmotion = (emotion: EmotionalCoordinates) => {
    if (emotion.arousal > 0.3 && emotion.valence < -0.3) return 'イライラ';
    if (emotion.arousal < -0.3 && emotion.valence < -0.3) return '落ち込み';
    if (emotion.valence < -0.3) return 'モヤモヤ';
    if (emotion.arousal < -0.3) return '疲れ';
    return 'モヤモヤ'; // デフォルト
  };

  return (
    <div className="min-h-screen bg-gradient-to-br from-purple-50 to-blue-50 p-4">
      <div className="max-w-2xl mx-auto">
        <div className="text-center mb-8">
          <h1 className="text-3xl font-bold text-gray-800 mb-2">
            今の気持ちを教えてください
          </h1>
          <p className="text-gray-600">
            感情の位置をタップして、あなたに最適な気晴らしを見つけましょう
          </p>
        </div>

        <div className="bg-white rounded-2xl shadow-xl p-6 mb-6">
          <EmotionCanvas
            onEmotionChange={setCurrentEmotion}
            onSelectionComplete={handleEmotionSelection}
          />
        </div>

        {isGenerating && (
          <div className="bg-white rounded-2xl shadow-xl p-6 text-center">
            <div className="animate-spin w-8 h-8 border-4 border-purple-500 border-t-transparent rounded-full mx-auto mb-4"></div>
            <p className="text-gray-600">あなたに最適な気晴らしを準備しています...</p>
          </div>
        )}

        {reframingContent && (
          <div className="bg-white rounded-2xl shadow-xl p-6">
            <h2 className="text-xl font-semibold text-gray-800 mb-4">
              {reframingContent.strategy.name}
            </h2>
            
            <div className="space-y-4">
              <div className="p-4 bg-purple-50 rounded-lg">
                <p className="text-gray-700">{reframingContent.personalizedScript}</p>
              </div>
              
              <div className="p-4 bg-blue-50 rounded-lg">
                <p className="text-sm font-medium text-blue-800 mb-2">呼吸のコツ:</p>
                <p className="text-blue-700">{reframingContent.breathingCue}</p>
              </div>
              
              <div className="p-4 bg-green-50 rounded-lg">
                <p className="text-sm font-medium text-green-800 mb-2">今日の言葉:</p>
                <p className="text-green-700 font-medium">{reframingContent.affirmation}</p>
              </div>

              <button className="w-full bg-purple-600 text-white py-3 px-6 rounded-lg font-medium hover:bg-purple-700 transition-colors">
                音声ガイドを開始
              </button>
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

function generateFallbackContent(emotion: EmotionalCoordinates) {
  return {
    strategy: { name: "基本的な心のケア" },
    personalizedScript: "今感じている気持ちをそのまま受け入れて、深呼吸をしてみましょう。",
    breathingCue: "4秒吸って、6秒で息を吐いてください。",
    affirmation: "今のあなたで十分です。"
  };
}
```

---

## 🚀 即座テスト実行

```bash
# 新機能のテスト実行
npm run dev

# 別ターミナルで単体テスト
npm test -- --watch --testPathPattern="cognitive-reframing|mood-input"

# ESLintチェック
npm run lint

# TypeScriptチェック  
npm run type-check

echo "🎉 Phase 4.1 基盤実装完了！ http://localhost:3000/reframing で確認"
```

---

## 📈 次のステップ

### 今日中に完了すべき作業
1. ✅ ネガティブ感情リフレーミングエンジンのプロトタイプ
2. ✅ ゼロUIムード入力の基本実装
3. ✅ Gemini統合テスト

### 明日の作業予定
1. 🔄 音声SSML生成の高度化
2. 🔄 効果測定システムの実装
3. 🔄 ユーザビリティテストの準備

### 1週間後の目標
1. 🎯 Phase 4.1 完全版リリース
2. 🎯 初期ユーザーテスト開始
3. 🎯 Phase 4.2 設計着手

**今すぐコードをコピーして実装を開始し、世界初の職場統合型マイクロウェルネス体験を実現しましょう！** 🚀